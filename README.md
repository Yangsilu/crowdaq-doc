# Crowdaq

Data are key to success for AI systems. However, large-scale data annotation efforts are often confronted with a set of common challenges: 
- designing a user-friendly annotation interface
- training enough annotators efficiently
- reproducibility

[Crowdaq](https://www.crowdaq.com/), short for crowdsourcing with automated qualification, is a web-based platform aimed to address these problems. It standardizes the data collection pipeline with customizable user-interface components, automated annotator qualification, and saved pipelines in a re-usable format.

# Overview

This repository hosts the documentation for Crowdaq. It has the following sections:
- Documentation for requesters (those who want to collect data) [link](docs/requester)
- Documentation for developers (those who want to contribute to, modify, and/or deploy Crowdaq) [link](docs/developer)
- API References [link](docs/api)

# EMNLP Reviewers

This work is currently in submission to EMNLP'2020 (demo track). If you are a reviewer for this submission, please check [here](docs/emnlp-reviewers) for additional information intended for you.