# Requester Documentation

This is the documentation for data requesters who want to use Crowdaq to improve their data collection workflows.

## Overview
As a highlight, what you can do on Crowdaq's website is:
- Build annotation interfaces declaratively without knowing HTML/CSS/JavaScript
- Design qualification exams declaratively without knowing how to set up a backend server
- Monitor how well people are doing on your qualification exams
- Monitor the annotation progress of your main tasksets
- Easily download necessarily files for reproducibility purpose

What you can do with Crowdaq's client package is:
- Upload/Download resource files to/from your Crowdaq account programmtically
- Launch your exams or tasksets on MTurk
- Manage your HITs on MTurk
- Calculate more advanced metrics for monitoring purpose


## Table of Content
1. [Get Started](get-started.md): We will use a running example to walk through the main features you will need to use on Crowdaq.
2. [Syntax](syntax.md): Detailed explanation of the syntax of instructions, tutorials, exams, and tasksets.
3. [Launch](launch.md): How to launch your exams and tasksets to the public, especially to MTurk
4. [Monitor](monitor.md): Built-in features that help you monitor the progress and quality of your data collection.
5. [Release](release.md): If you want to release a dataset collected on Crowdaq, what're the things to publish so that future requesters can easily reproduce or extend your dataset.